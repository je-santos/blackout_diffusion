{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy.optimize import bisect\n",
    "from scipy.stats import binom\n",
    "\n",
    "from utils import save_checkpoint_withEval as save_checkpoint\n",
    "from utils import restore_checkpoint_withEval as restore_checkpoint\n",
    "from loadDataPipeline import generateData\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"score_sde_pytorch/\")\n",
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "# from score_sde_pytorch.models import ncsnpp\n",
    "from configs.vp import cifar10_ncsnpp_continuous as configLoader\n",
    "from models import utils as mutils\n",
    "from models.ema import ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the ML model from Song et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configLoader.get_config()\n",
    "config.training.batch_size = 128\n",
    "config.training.snapshot_freq_for_preemption = 10\n",
    "config.training.snapshot_freq = 50_000\n",
    "config.training.log_freq = 100\n",
    "\n",
    "config.data.dataset = \"CELEBA\"\n",
    "config.data.image_size = 64\n",
    "config.data.data_path = \"/project/smartFRACs/jesantos/generative-discrete-state-diffusion-models/dataBuffer/celebA/celebA_64_64.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.num_scales = 1_000\n",
    "config.model.num_bins = 256\n",
    "config.model.t_end = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for observation times (noise schedule) and forward solution or directly loading previously saved ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.log(x / (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blackout_loader(Dataset):\n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.T = config.model.num_scales\n",
    "        self.t_end = config.model.t_end\n",
    "        self.im_size = config.data.image_size\n",
    "        self.batch_size = config.training.batch_size\n",
    "        self.num_bins = 256\n",
    "\n",
    "        self.forward_solution()\n",
    "        self.compute_weights()\n",
    "        self.load_data()\n",
    "\n",
    "    def forward_solution(self):\n",
    "\n",
    "        x_end = np.exp(-self.t_end)\n",
    "        f_grid = np.linspace(-f(x_end), f(x_end), self.T)\n",
    "        x_grid = np.array(\n",
    "            [\n",
    "                bisect(lambda x: f(x) - f_grid[i], x_end / 2, 1 - x_end / 2)\n",
    "                for i in range(self.T)\n",
    "            ]\n",
    "        )\n",
    "        self.observation_times = -np.log(x_grid)\n",
    "\n",
    "        table = np.zeros((self.num_bins, self.num_bins))\n",
    "        for n in range(self.num_bins):\n",
    "            for m in range(n):\n",
    "                table[n, m] = n - m\n",
    "            table[n, n] = 0\n",
    "        self.table = np.repeat(table[:, :, None], self.T, axis=-1).flatten()\n",
    "\n",
    "        support = np.arange(self.num_bins)\n",
    "        sol = np.zeros((self.T + 1, self.num_bins, self.num_bins))\n",
    "        sol[0, :, :] = np.eye(self.num_bins)\n",
    "\n",
    "        self.pt = np.exp(-self.observation_times)\n",
    "        for t in range(self.T):\n",
    "            for i in range(self.num_bins):\n",
    "                sol[t + 1, :, i] = binom(i, self.pt[t]).pmf(support)\n",
    "        self.cumulative = np.cumsum(sol, axis=1)\n",
    "\n",
    "    def compute_weights(self):\n",
    "\n",
    "        self.sampling_prob = np.ones_like(self.pt) / np.sum(np.ones_like(self.pt))\n",
    "        self.weights = (self.pt * np.diff(self.observation_times, prepend=0) / self.sampling_prob)\n",
    "        \n",
    "    def load_data(self):\n",
    "\n",
    "        self.data = np.load(config.data.data_path).transpose((0, 3, 1, 2))\n",
    "        self.training_ims = self.data.shape[0]\n",
    "        self.num_batches = int(self.training_ims / self.batch_size)\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ims_ind = torch.randperm(self.training_ims)[: self.batch_size]\n",
    "        ims = self.data[ims_ind]\n",
    "\n",
    "        t_ind = np.random.choice(self.T, size=(self.batch_size, 1, 1, 1), p=self.sampling_prob)\n",
    "        cp = self.cumulative[t_ind + 1, :, ims]\n",
    "        u = np.random.uniform(size=(self.batch_size, 3, self.im_size, self.im_size, 1))\n",
    "        nt = np.argmax(u < cp, axis=4)\n",
    "\n",
    "        index = self.T * (self.num_bins * ims + nt) + t_ind\n",
    "\n",
    "        birth_rate = self.table[index]\n",
    "        mean_v = 127.5 * self.pt[t_ind]\n",
    "\n",
    "        target = nt - mean_v\n",
    "\n",
    "        return target, birth_rate, t_ind, self.weights[t_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    blackout_loader(config),\n",
    "    batch_size=None,\n",
    "    pin_memory=True,\n",
    "    num_workers=16,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_ch(tensor):\n",
    "    if len(tensor.shape) == 3:\n",
    "        return tensor.permute(1, 2, 0)\n",
    "    elif len(tensor.shape) == 4:\n",
    "        return tensor.permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims, birth_rate, t, w = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ims = swap_ch(ims)\n",
    "birth_rate = swap_ch(birth_rate)\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    im = swap_ch( torch.as_tensor( train_loader.dataset.data[i,] ) )\n",
    "    out_im = 127.5*( ims[i,]+1 )\n",
    "    br = birth_rate[i,]\n",
    "    ti = t[i] \n",
    " \n",
    "    fig, ax = plt.subplots(1,3, figsize=(4.8,1.5))\n",
    "    \n",
    "    ax[0].imshow(im)\n",
    "    ax[1].imshow(out_im)\n",
    "    ax[1].set_title(f't={ti.item()}')\n",
    "    ax[2].imshow(br)\n",
    "    \n",
    "    \n",
    "    [ax[j].set_xticklabels('') and ax[j].set_yticklabels('') for j in range(3)]\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an ML model to learn the transition rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"blackout-celebA64\"\n",
    "\n",
    "score_model = mutils.create_model(config)\n",
    "score_fn = mutils.get_model_fn(score_model, train=True)\n",
    "optimizer = torch.optim.Adam(score_model.parameters(), lr=config.optim.lr)\n",
    "\n",
    "ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n",
    "\n",
    "state = dict(\n",
    "    optimizer=optimizer,\n",
    "    model=score_model,\n",
    "    ema=ema,\n",
    "    lossHistory=[],\n",
    "    evalLossHistory=[],\n",
    "    step=0,\n",
    ")\n",
    "\n",
    "checkpoint_dir = os.path.join(workdir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(workdir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "\n",
    "tf.io.gfile.makedirs(checkpoint_dir)\n",
    "tf.io.gfile.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "state = restore_checkpoint(checkpoint_meta_dir, state, config.device)\n",
    "\n",
    "initial_step = int(state[\"step\"])\n",
    "loss_history = state[\"loss_history\"]\n",
    "eval_history = state[\"eval_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "softplus = torch.nn.functional.softplus\n",
    "iterloader = iter(train_loader)\n",
    "\n",
    "for step in range(initial_step, config.training.n_iters):\n",
    "\n",
    "    ims, birth_rate, t, w = next(iterloader)\n",
    "    optimizer.zero_grad()\n",
    "    y = softplus(score_fn(ims, t))\n",
    "    loss = torch.mean(w * (y - birth_rate * torch.log(y)))\n",
    "    loss.backward()\n",
    "\n",
    "    state[\"ema\"].update(state[\"model\"].parameters())\n",
    "\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.detach())\n",
    "\n",
    "    if step != 0 and step % config.training.snapshot_freq_for_preemption == 0:\n",
    "        save_checkpoint(checkpoint_meta_dir, state)\n",
    "\n",
    "    if (\n",
    "        step != 0\n",
    "        and step % config.training.snapshot_freq == 0\n",
    "        or step == config.training.n_iters\n",
    "    ):\n",
    "        save_step = step // config.training.snapshot_freq\n",
    "        save_checkpoint(os.path.join(checkpoint_dir, f\"checkpoint_{save_step}.pth\"), state)\n",
    "\n",
    "    if np.mod(step, config.training.log_freq) == 0:\n",
    "\n",
    "        ema.store(score_model.parameters())\n",
    "        ema.copy_to(score_model.parameters())\n",
    "\n",
    "        y = softplus(score_fn(ims, t))\n",
    "        loss = torch.mean(w * (y - birth_rate * torch.log(y)))\n",
    "\n",
    "        ema.restore(score_model.parameters())\n",
    "        eval_history.append(loss.detach())\n",
    "\n",
    "        print(f\"iter: {step}, loss: {loss_history[-1]}, eval loss: {eval_history[-1]}\")\n",
    "\n",
    "    state[\"step\"] = step\n",
    "    state[\"loss_history\"] = loss_history\n",
    "    state[\"eval_history\"] = eval_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch] *",
   "language": "python",
   "name": "conda-env-.conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
